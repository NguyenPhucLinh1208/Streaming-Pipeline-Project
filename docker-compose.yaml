x-airflow-common:
# đây là cấu hình chung của airflow, thực tế cần mount Airflow vào để đồng bộ dags, lên lịch, web.
  &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.3}  
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflowmeta@host.docker.internal:5432/airflow_spark
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__RUN_DURATION: 30
    PYTHONPATH: /usr/local/lib/python3.12/site-packages  # Thiết lập PYTHONPATH
    
  volumes:
    - ./Airflow:/opt/airflow # mount thế này mới đúng, vì cấu hình mặc định trong container của airflow chứa dags trong này
    - ./Kafka:/opt/Kafka
    - ./Spark:/opt/Spark
    - shared-libs:/usr/local/lib/python3.12/site-packages
  user: "0:0"

services:

# Spark service.
  spark_project:
    build:
      context: .
      dockerfile: .devcontainer/Dockerfile
    volumes:
      - shared-libs:/usr/local/lib/python3.12/site-packages
    networks:
      - project-network

# cấu hình các dịch vụ airflow.
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks:
      - project-network

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    restart: always
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks:
      - project-network

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        exec /entrypoint airflow db init
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflowSpark}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflowSpark}
    user: "0:0"
    depends_on: 
      - "spark_project"  # để lấy đúng danh sách thư viện, cần container này chạy đầu tiên
    networks:
      - project-network

# cấu hình kafka.
  zookeeper:
    image: confluentinc/cp-zookeeper:7.1.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: "2181"
    networks:
      - project-network
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_txn_logs:/var/lib/zookeeper/log
    depends_on:
      - "spark_project"


  kafka0:
    image: confluentinc/cp-kafka
    ports:
    - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 0
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka0:9093,EXTERNAL://host.docker.internal:9092"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_LOG_RETENTION_HOURS: 8                 # Giữ dữ liệu 8 giờ (thời gian giao dịch)
      KAFKA_LOG_RETENTION_BYTES: 500000000          # Giới hạn dung lượng
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
    depends_on:
      - "zookeeper"
    networks:
      - project-network
    volumes:
      - kafka_data:/var/lib/kafka/data

  schema-registry:
    image: confluentinc/cp-schema-registry
    ports:
    - "8081:8081"
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "PLAINTEXT://kafka0:9093"
      SCHEMA_REGISTRY_LISTENERS: "http://0.0.0.0:8081"
      SCHEMA_REGISTRY_HOST_NAME: "schema-registry"
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: "1"
    depends_on:
      - "kafka0"
    networks:
      - project-network

  connect:
    image: confluentinc/cp-kafka-connect
    ports:
    - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "PLAINTEXT://kafka0:9093"
      CONNECT_GROUP_ID: "connect"
      CONNECT_REST_ADVERTISED_HOST_NAME: "connect"
      CONNECT_PLUGIN_PATH: "/usr/share/java"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_KEY_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      CONNECT_CONFIG_STORAGE_TOPIC: "connect-config"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offset"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
    depends_on:
      - "kafka0"
      - "schema-registry"
    networks:
      - project-network
    volumes:
      - shared-libs:/usr/local/lib/python3.12/site-packages
      - ./Kafka:/opt/Kafka

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    hostname: spark-master
    environment:
      - SPARK_MASTER_WEBUI_PORT=8088
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8088:8088" # Spark Web UI
      - "7077:7077" # Spark Master Port
      - "6066:6066" # Spark Master REST Port
    depends_on:
      - "spark_project"
    volumes:
      - data:/opt/bitnami/spark/data
      - ./Spark:/opt/Spark
    networks:
      - project-network
    user: "0:0"


  spark-worker-1:
    image: bitnami/spark:latest
    container_name: spark-worker-1
    hostname: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    volumes:
      - data:/opt/bitnami/spark/data
      - ./Spark:/opt/Spark
    networks:
      - project-network
    user: "0:0"

  spark-worker-2:
    image: bitnami/spark:latest
    container_name: spark-worker-2
    hostname: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    volumes:
      - data:/opt/bitnami/spark/data
      - ./Spark:/opt/Spark
    networks:
      - project-network
    user: "0:0"


networks:
  project-network:

volumes:
  shared-libs:
  data:
  kafka_data:
  zookeeper_data:
  zookeeper_txn_logs:


